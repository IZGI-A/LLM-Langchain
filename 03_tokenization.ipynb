{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK-lZSBx-wfu"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "\n",
        "\n",
        "Tokenization or [Text segmentation](https://en.wikipedia.org/wiki/Text_segmentation) is the problem of dividing a string of written language into its component words.\n",
        "\n",
        "The most simple way to divide a text into a list of its words is to split over the whitespaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYWmSVm4-wfv",
        "outputId": "62441624-1daf-4655-8008-e3623fb3998b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's\", 'eat,', 'grandpa']\n"
          ]
        }
      ],
      "source": [
        "text = \"Let's eat, grandpa\"\n",
        "print(text.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1m6rvCf-wfv"
      },
      "source": [
        "The problem with that approach is that contractions (Let's -> Let + s) are not handled and punctuations signs stay attached to the nearest word (eat, -> eat + ,)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zgD0o6v-wfw"
      },
      "source": [
        "The right way to tokenize is to use a tokenizer. Most NLP libraries offer their own tokenizers. Here we will use tokenizers from the [NLTK](https://www.nltk.org/) library.\n",
        "\n",
        "The NLTK library offers many [tokenizers](https://www.nltk.org/api/nltk.tokenize.html). We'll work with the WordPunctTokenizer.\n",
        "\n",
        "But first let's install NLTK and download the necessary resources.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B4Q8pWe-wfw",
        "outputId": "f7fb186e-bda7-42b0-9006-f53f2b178b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tx9EOzx-wfw",
        "outputId": "fc82a1de-a3f5-4eda-c6a9-a0e8ffc43ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d_Ypwp0-wfw"
      },
      "source": [
        "# Apply the WordPunctTokenizer\n",
        "\n",
        "We get a different results. The punctuations are now handled as tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfFwBkOm-wfw",
        "outputId": "acbf24c2-9684-46f9-839e-0d8264d0a58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'\", 's', 'eat', 'your', 'soup', ',', 'Grandpa', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokens = WordPunctTokenizer().tokenize(\"Let's eat your soup, Grandpa.\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zoKKEFz-wfw"
      },
      "source": [
        "Let's tokenize the text from the Wikipedia Earth page and look at the frequency of the most common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oba1MBQV-wfw"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "def wikipedia_page(title):\n",
        "    '''\n",
        "    This function returns the raw text of a wikipedia page\n",
        "    given a wikipedia page title\n",
        "    '''\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json', # request json formatted content\n",
        "        'titles': title, # title of the wikipedia page\n",
        "        'prop': 'extracts',\n",
        "        'explaintext': True\n",
        "    }\n",
        "    # send a request to the wikipedia api\n",
        "    response = requests.get(\n",
        "         'https://en.wikipedia.org/w/api.php',\n",
        "         params= params\n",
        "     ).json()\n",
        "\n",
        "    # Parse the result\n",
        "    page = next(iter(response['query']['pages'].values()))\n",
        "    # return the page content\n",
        "    if 'extract' in page.keys():\n",
        "        return page['extract']\n",
        "    else:\n",
        "        return \"Page not found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9GFhGll-wfx",
        "outputId": "418131c4-9000-4a2a-a45b-fb379cbb0038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 736), (',', 589), ('.', 490), ('of', 362), ('and', 288), ('earth', 261), ('is', 174), ('to', 165), ('s', 160), (\"'\", 159), ('in', 157), ('a', 140), ('(', 113), ('-', 78), ('by', 77), ('as', 74), ('with', 73), ('from', 70), ('surface', 66), ('at', 59)]\n"
          ]
        }
      ],
      "source": [
        "text = wikipedia_page('Earth').lower()\n",
        "tokens = WordPunctTokenizer().tokenize(text)\n",
        "print(Counter(tokens).most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBArL6Ex-wfx"
      },
      "source": [
        "We now see that earth and earth's for instance are no longer separate tokens and that the punctuation signs are stand alone tokens. This will come in handy if we want to remove them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou6YWlKI-wfx"
      },
      "source": [
        "# Tokenization on characters\n",
        "\n",
        "We can also tokenize on characters instead of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTFl6Bnw-wfx",
        "outputId": "ac40a03e-88fb-48e6-9d64-d66eff032e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common characters in the text\n",
            "[(' ', 8960), ('e', 5579), ('t', 4310), ('a', 4142), ('i', 3293), ('o', 3229), ('s', 3115), ('r', 3059), ('n', 3033), ('h', 2241), ('l', 2022), ('c', 1575), ('d', 1475), ('m', 1300), ('u', 1164), ('f', 951), ('p', 943), ('g', 841), ('y', 676), (',', 632)]\n",
            "\n",
            "All characters in the text: \n",
            "{'p', 'l', '1', 'x', '×', '−', 'n', 'ē', 'þ', '[', 'w', 'v', '-', 'i', 'ɡ', 'a', '2', 'µ', '*', 'y', 'k', '4', '0', '—', '±', '=', 'ć', 'c', 'f', '5', 'á', 't', 'â', '(', '6', 'ῖ', 'q', 'ū', 'ð', ')', 'e', '–', 'α', 'h', 'ʻ', 'u', '8', \"'\", 'ñ', '+', 'o', 'γ', '\"', '3', 'j', '\\n', 's', '7', '%', ':', '/', '̯', 'ö', ']', 'æ', 'č', 'r', '°', ',', ';', 'd', 'ō', '?', ' ', 'b', '.', 'm', 'z', '9', 'g', 'ῆ'}\n"
          ]
        }
      ],
      "source": [
        "# example of character tokenization\n",
        "char_tokens = [ c for c in text ]\n",
        "print(\"Most common characters in the text\")\n",
        "print(Counter(char_tokens).most_common(20))\n",
        "print()\n",
        "print(f\"All characters in the text: \\n{set(char_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx7jFgIA-wfx"
      },
      "source": [
        "# N-grams\n",
        "\n",
        "\n",
        "Some words are better taken together: New York, Happy end, Wall street, Linear regression etc ... . When tokenizing we want to consider all possible adjacent pairs of words in the text. We can do this with the NLTK ngrams function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoV9wsDM-wfx",
        "outputId": "91bbb038-ca5d-48cf-fbca-a0ddda903d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('how', 'much'), ('much', 'wood'), ('wood', 'would'), ('would', 'a'), ('a', 'woodchuck'), ('woodchuck', 'chuck'), ('chuck', 'if'), ('if', 'a'), ('a', 'woodchuck'), ('woodchuck', 'could'), ('could', 'chuck'), ('chuck', 'wood'), ('wood', '?')]\n",
            "\n",
            "['how_much', 'much_wood', 'wood_would', 'would_a', 'a_woodchuck', 'woodchuck_chuck', 'chuck_if', 'if_a', 'a_woodchuck', 'woodchuck_could', 'could_chuck', 'chuck_wood', 'wood_?']\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "text = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\".lower()\n",
        "\n",
        "# Tokenize\n",
        "tokens = WordPunctTokenizer().tokenize(text)\n",
        "\n",
        "# bigrams\n",
        "bigrams = [w for w in  ngrams(tokens,n=2)]\n",
        "print(bigrams)\n",
        "\n",
        "print()\n",
        "bigrams = ['_'.join(bg) for bg in bigrams]\n",
        "print(bigrams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqiK0Tp4-wfx",
        "outputId": "51a9e740-90d5-4470-8e31-3fe36c6b181f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['how_much_wood', 'much_wood_would', 'wood_would_a', 'would_a_woodchuck', 'a_woodchuck_chuck', 'woodchuck_chuck_if', 'chuck_if_a', 'if_a_woodchuck', 'a_woodchuck_could', 'woodchuck_could_chuck', 'could_chuck_wood', 'chuck_wood_?']\n"
          ]
        }
      ],
      "source": [
        "# and for trigrams\n",
        "\n",
        "trigrams = ['_'.join(w) for w in  ngrams(tokens,n=3)]\n",
        "\n",
        "print(trigrams)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtYtXaIJ-wfx"
      },
      "source": [
        "# add ngrams to list of tokens\n",
        "Let's add the bigrams and trigrams to the list of tokens on the wikipedia Earth page and look at the frequency of ngrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHTocxYu-wfx"
      },
      "outputs": [],
      "source": [
        "text = wikipedia_page('Earth').lower()\n",
        "unigrams = WordPunctTokenizer().tokenize(text)\n",
        "bigrams = ['_'.join(w) for w in  ngrams(unigrams,n=2)]\n",
        "trigrams = ['_'.join(w) for w in  ngrams(unigrams,n=3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQGeJKHk-wfx"
      },
      "outputs": [],
      "source": [
        "tokens = unigrams + bigrams + trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVSWixhq-wfx",
        "outputId": "be7a598b-c9a6-4477-f8c1-20dee98a1454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we have a total of 33798 tokens, including: \n",
            "- 11267 unigrams \n",
            "- 11266 bigrams \n",
            "- 11265 trigrams. \n"
          ]
        }
      ],
      "source": [
        "print(f\"we have a total of {len(tokens)} tokens, including: \\n- {len(unigrams)} unigrams \\n- {len(bigrams)} bigrams \\n- {len(trigrams)} trigrams. \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O42L-7NS-wfx",
        "outputId": "f71e5fa8-5ccb-49ba-c96a-27f41c71e1d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 736),\n",
              " (',', 589),\n",
              " ('.', 490),\n",
              " ('of', 362),\n",
              " ('and', 288),\n",
              " ('earth', 261),\n",
              " ('is', 174),\n",
              " ('to', 165),\n",
              " ('s', 160),\n",
              " (\"'\", 159),\n",
              " (\"'_s\", 159),\n",
              " ('in', 157),\n",
              " ('a', 140),\n",
              " (\"earth_'\", 137),\n",
              " (\"earth_'_s\", 137),\n",
              " ('(', 113),\n",
              " ('of_the', 98),\n",
              " ('._the', 81),\n",
              " ('-', 78),\n",
              " ('by', 77),\n",
              " ('as', 74),\n",
              " ('with', 73),\n",
              " ('from', 70),\n",
              " (',_and', 67),\n",
              " ('surface', 66),\n",
              " ('at', 59),\n",
              " ('that', 58),\n",
              " ('in_the', 56),\n",
              " (')', 54),\n",
              " (',_the', 53),\n",
              " ('water', 52),\n",
              " ('of_earth', 51),\n",
              " ('sun', 50),\n",
              " ('are', 49),\n",
              " ('it', 46),\n",
              " ('about', 44),\n",
              " ('the_sun', 43),\n",
              " ('===', 42),\n",
              " ('to_the', 42),\n",
              " ('this', 41),\n",
              " ('its', 41),\n",
              " ('atmosphere', 41),\n",
              " ('on', 41),\n",
              " ('solar', 40),\n",
              " ('land', 40),\n",
              " ('which', 36),\n",
              " ('crust', 36),\n",
              " ('has', 36),\n",
              " ('million', 36),\n",
              " (\"of_earth_'\", 36)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Counter(tokens).most_common(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwvWwIIH-wfx"
      },
      "source": [
        "We have multiple bigrams in the top 50 tokens:\n",
        "\n",
        "- of_the\n",
        "- of_earth\n",
        "- in_the\n",
        "\n",
        "Adding ngrams to a list of tokens may help down the line when classifying text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRzZHav3-wfx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}